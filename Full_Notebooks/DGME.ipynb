{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This code implements the DGME method proposed and analyzed in the paper ``DEEP BACKWARD AND GALERKIN METHODS FOR LEARNING FINITE\n",
        "STATE MASTER EQUATIONS'' by Asaf Cohen, Mathieu Lauri√®re and Ethan Zell.\n",
        "\n",
        "The example solved here corresponds to Example 7.1 in the paper."
      ],
      "metadata": {
        "id": "kDd7Av0jslX5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TD3cgWTo0DGP"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import math, keras, random, copy\n",
        "tf.random.set_seed(703)\n",
        "np.random.seed(703)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJxZMGf-0tHI"
      },
      "outputs": [],
      "source": [
        "# global, static parameters\n",
        "d=2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AueZXhCugOxt"
      },
      "outputs": [],
      "source": [
        "class DGMDatasetGenerator4:\n",
        "  '''\n",
        "  A class that generates the dataset for the DGM.\n",
        "  '''\n",
        "  def __init__(self, d=d, horizon=1.):\n",
        "    self.t_data = None\n",
        "    self.x_data = None\n",
        "    self.eta_data = None\n",
        "    self.t_terminal_data = None\n",
        "    self.x_terminal_data = None\n",
        "    self.eta_terminal_data = None\n",
        "    self.d = d\n",
        "    self.horizon = horizon\n",
        "\n",
        "  def uniformly_random_measure(self):\n",
        "    '''\n",
        "    Uses exponential random variables to generate a uniformly random probability vector.\n",
        "    '''\n",
        "    pre_normalized = np.random.exponential(1, size = (self.d,))\n",
        "    return pre_normalized / sum(pre_normalized)\n",
        "\n",
        "  def uniformly_random_measure_vec(self, samples):\n",
        "    '''\n",
        "    Vectorizes the prior function.\n",
        "    '''\n",
        "    data = np.zeros((samples, self.d))\n",
        "    for k in range(samples):\n",
        "      data[k,:] = self.uniformly_random_measure()\n",
        "    return data\n",
        "\n",
        "  def generate_dataset(self, samples=1000, terminal_samples = 100):\n",
        "\n",
        "    '''\n",
        "    Creates the dataset which, for the DGME, needs t, x, and eta data.\n",
        "    '''\n",
        "\n",
        "    t_data = np.random.uniform(0,self.horizon,size=samples)\n",
        "    x_data = np.random.choice([float(m) for m in range(self.d)],size=samples)\n",
        "    eta_data = self.uniformly_random_measure_vec(samples = samples)\n",
        "\n",
        "    t_terminal_data = np.full(shape = (terminal_samples,), fill_value = self.horizon)\n",
        "    x_terminal_data = np.random.choice([float(m) for m in range(self.d)], size=terminal_samples)\n",
        "    eta_terminal_data = self.uniformly_random_measure_vec(samples = terminal_samples)\n",
        "\n",
        "    self.t_data = t_data\n",
        "    self.x_data = x_data\n",
        "    self.eta_data = eta_data\n",
        "\n",
        "    self.t_terminal_data = t_terminal_data\n",
        "    self.x_terminal_data = x_terminal_data\n",
        "    self.eta_terminal_data = eta_terminal_data\n",
        "\n",
        "    return t_data, x_data, eta_data, t_terminal_data, x_terminal_data, eta_terminal_data\n",
        "\n",
        "  def oversampling(self, oversample_T = True):\n",
        "    '''\n",
        "    Apply this function after generate_dataset to modify the domain of the sampled eta and T.\n",
        "    Sampling outside the domain may improve performance along the domain's boundary.\n",
        "    '''\n",
        "    if oversample_T:\n",
        "      terminal_samples = self.t_terminal_data.shape[0]\n",
        "\n",
        "      random_mask = np.full(terminal_samples, False)\n",
        "      random_mask[:int(terminal_samples * 0.5)] = True # this will make half of them oversampling\n",
        "      np.random.shuffle(random_mask)\n",
        "\n",
        "      self.t_terminal_data[random_mask == True] += np.random.uniform(0, self.horizon, size = int(terminal_samples * 0.5))\n",
        "\n",
        "    return  self.t_data, self.x_data, self.eta_data, self.t_terminal_data, self.x_terminal_data, self.eta_terminal_data\n",
        "\n",
        "  def data_to_tensors(self):\n",
        "\n",
        "    self.t_data = tf.convert_to_tensor(self.t_data, dtype = 'float32')\n",
        "    self.x_data = tf.convert_to_tensor(self.x_data, dtype = 'float32')\n",
        "    self.eta_data = tf.convert_to_tensor(self.eta_data, dtype = 'float32')\n",
        "    self.t_terminal_data = tf.convert_to_tensor(self.t_terminal_data, dtype = 'float32')\n",
        "    self.x_terminal_data = tf.convert_to_tensor(self.x_terminal_data, dtype = 'float32')\n",
        "    self.eta_terminal_data = tf.convert_to_tensor(self.eta_terminal_data, dtype = 'float32')\n",
        "\n",
        "    return  self.t_data, self.x_data, self.eta_data, self.t_terminal_data, self.x_terminal_data, self.eta_terminal_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X90_-Q68M9E2"
      },
      "outputs": [],
      "source": [
        "class DGMModel(tf.keras.Model):\n",
        "  '''\n",
        "  This class defines the neural network model.\n",
        "  '''\n",
        "  def __init__(self, architecture):\n",
        "    super(DGMModel, self).__init__()\n",
        "    self.architecture = architecture # you can give a list specifying the number of nodes in each dense layer\n",
        "    self.layer_list = []\n",
        "\n",
        "    for i,number_of_nodes in enumerate(architecture):\n",
        "      if i == 0:\n",
        "        self.layer_list.append(tf.keras.layers.Dense(units=number_of_nodes, activation='sigmoid',\n",
        "                                                              kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.),\n",
        "                                                              bias_initializer='zeros'))\n",
        "      else:\n",
        "        self.layer_list.append(tf.keras.layers.Dense(units=number_of_nodes, activation='sigmoid',\n",
        "                                                              kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.),\n",
        "                                                              bias_initializer='zeros'))\n",
        "    self.layer_list.append(tf.keras.layers.Dense(units=1, activation = 'elu'))\n",
        "\n",
        "  def call(self, t , x, eta): # changed the concatenation because eta changed?\n",
        "    t = tf.expand_dims(t, axis = -1)\n",
        "    x = tf.expand_dims(x, axis = -1)\n",
        "    input = tf.concat([t, x, eta], 1)\n",
        "    result = input\n",
        "    for layer in self.layer_list:\n",
        "      result = layer(result)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall from the paper that we are interested in approximately solving the master equation:\n",
        "\n",
        "$$\n",
        "\\partial_t U(t,x,\\eta) = H(x,\\Delta_x U(t,\\cdot,\\eta))+ F(x,\\eta) + \\sum_{y,z\\in [d]} D^\\eta_{yz} U(t,x,\\eta) \\gamma^*_z(y,\\Delta_y U(t,\\cdot,\\eta)) \\eta_y,\n",
        "$$\n",
        "\n",
        "where in this example:\n",
        "\n",
        "$$F(x,\\eta) = \\eta_x,$$\n",
        "\n",
        "$$H(x,p) := \\min_{a} \\Big\\{\\frac{1}{2}|a|^2 + a\\cdot p\\Big\\},$$\n",
        "\n",
        "and where $\\gamma^*$ is the associated minimal argument that minimizes the Hamiltonian $H$. Recall that $\\Delta_x b:= (b_y - b_x)_{y\\in [d]}$ is a finite difference vector and $D^\\eta_{yz}$ denotes the directional derivative in the $z$ minus $y$ direction (in terms of the standard basis).\n",
        "\n",
        "In the Loss class below, $F$ is referred to as the mean_field_cost and $H$ is the Hamiltonian."
      ],
      "metadata": {
        "id": "2YU6IVS7-b2V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvCpCg0YMCbm"
      },
      "outputs": [],
      "source": [
        "class Loss():\n",
        "  '''\n",
        "  This class defines the loss and involves the entire PDE.\n",
        "  '''\n",
        "\n",
        "  def __init__(self, model, d, a = 2., a_l = 1., a_u = 3., b = 4.):\n",
        "    self.model = model\n",
        "    self.d = d\n",
        "    self.a = a\n",
        "    self.a_l = a_l\n",
        "    self.a_u = a_u\n",
        "    self.b = b\n",
        "    return\n",
        "\n",
        "  def a_star(self, psi_left, psi_right):\n",
        "    '''\n",
        "    The computed value of $\\gamma^*$, the minimal argument of the Hamiltonian. The paper derives this formula explicitly.\n",
        "    '''\n",
        "    numerator = psi_left - psi_right\n",
        "    return ( numerator / ( (self.a_u - self.a_l) * self.b) ) + self.a\n",
        "\n",
        "  def Hamiltonian_z1(self, x, z1_state_tensor, psi_output, psi_z1_output):\n",
        "    '''\n",
        "    The Hamiltonian, denoted H in the paper.\n",
        "    '''\n",
        "    a_star_z1 = self.a_star(psi_output, psi_z1_output)\n",
        "    where_unequal = tf.cast(tf.math.logical_not(tf.math.equal(x, z1_state_tensor)), dtype = 'float32')\n",
        "    pre_running = a_star_z1 - ( self.a * tf.ones(shape = a_star_z1.shape, dtype='float32') )\n",
        "    running_cost = self.b * tf.multiply(tf.math.square(pre_running), where_unequal)\n",
        "    change_of_state = tf.multiply(a_star_z1, (psi_z1_output - psi_output))\n",
        "    return running_cost + change_of_state\n",
        "\n",
        "  def interaction_sum_term_z1_z2(self, z1, z2, eta, eta_derivative, psi_z1, psi_z2):\n",
        "    '''\n",
        "    This is an individual term from the sum part of the PDE.\n",
        "    '''\n",
        "    mf = eta[:,int(z1)]\n",
        "    directional_derivative = eta_derivative[:,int(z2)] - eta_derivative[:,int(z1)]\n",
        "    control = self.a_star(psi_z1, psi_z2)\n",
        "    result = tf.multiply(tf.multiply(mf, directional_derivative), control)\n",
        "    return result\n",
        "\n",
        "  def mean_field_cost(self, x, eta):\n",
        "    '''\n",
        "    The common cost, denoted F in the paper.\n",
        "    '''\n",
        "    mf = np.zeros(x.shape)\n",
        "    for i,entry in enumerate(x):\n",
        "      mf[i] = eta[i, int(entry)]\n",
        "    mfc = tf.convert_to_tensor(mf, dtype='float32')\n",
        "    return mfc\n",
        "\n",
        "  def derivatives(self, t, x, eta):\n",
        "\n",
        "    '''\n",
        "    The time and measure derivatives are needed to compute the interaction_sum_term_z1_z2 function. This then goes into the sum term of the PDE.\n",
        "    '''\n",
        "\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "      tape.watch(t)\n",
        "      V = self.model(t,x,eta)\n",
        "    time_derivative = tape.gradient(V,t)\n",
        "\n",
        "    with tf.GradientTape(persistent=True) as tape2:\n",
        "        tape2.watch(eta)\n",
        "        V = self.model(t,x,eta)\n",
        "    eta_derivative = tape2.gradient(V,eta)\n",
        "\n",
        "    self.time_derivative = time_derivative\n",
        "    self.eta_derivative = eta_derivative\n",
        "\n",
        "    return time_derivative, eta_derivative\n",
        "\n",
        "  def criterion(self, t, x, eta):\n",
        "\n",
        "    '''\n",
        "    Combining all the prior terms into the loss. We need two for loops in order to compute the sum term.\n",
        "    '''\n",
        "\n",
        "    time_derivative, eta_derivative = self.derivatives(t, x, eta)\n",
        "    output = tf.squeeze(self.model(t, x, eta))\n",
        "\n",
        "    sum = tf.zeros(t.shape)\n",
        "    for z1 in range(d):\n",
        "\n",
        "      z1_state_tensor = z1 * tf.ones(shape = t.shape, dtype='float32')\n",
        "      psi_z1 = tf.squeeze(self.model(t, z1_state_tensor, eta))\n",
        "      sum += self.Hamiltonian_z1(x, z1_state_tensor, output, psi_z1)\n",
        "\n",
        "      for z2 in range(d):\n",
        "\n",
        "        z2_state_tensor = z2 * tf.ones(shape = t.shape, dtype='float32')\n",
        "        psi_z2 = tf.squeeze(self.model(t, z2_state_tensor, eta))\n",
        "        term = self.interaction_sum_term_z1_z2(z1, z2, eta, eta_derivative, psi_z1, psi_z2)\n",
        "        sum += self.interaction_sum_term_z1_z2(z1, z2, eta, eta_derivative, psi_z1, psi_z2)\n",
        "\n",
        "    mean_field_cost = self.mean_field_cost(x, eta)\n",
        "\n",
        "    loss_sum = time_derivative + sum + mean_field_cost\n",
        "\n",
        "    squared_loss = tf.math.square(loss_sum)\n",
        "\n",
        "    return squared_loss # not yet reduced\n",
        "\n",
        "  def terminal_criterion(self, t_terminal, x_terminal, eta_terminal):\n",
        "\n",
        "    '''\n",
        "    We have an additional function to compute the loss at the terminal time.\n",
        "    '''\n",
        "\n",
        "    terminal_output = tf.squeeze(self.model(t_terminal, x_terminal, eta_terminal))\n",
        "    squared_terminal = tf.math.square(terminal_output)\n",
        "\n",
        "    return squared_terminal\n",
        "\n",
        "  def total_criterion(self, t, x, eta, t_terminal, x_terminal, eta_terminal, factor=1.):\n",
        "    unreduced_loss = self.criterion(t, x, eta)\n",
        "    unreduced_terminal_loss = self.terminal_criterion(t_terminal, x_terminal, eta_terminal)\n",
        "    loss = tf.reduce_mean(unreduced_loss)\n",
        "    terminal_loss = tf.reduce_mean(unreduced_terminal_loss)\n",
        "    return loss + (factor * terminal_loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuFERhYa1x_t"
      },
      "outputs": [],
      "source": [
        "class Train():\n",
        "  def __init__(self, model, dataset_generator, b=1., factor = 1., oversampling = True, return_losses = False, verbose = False, visual_output = False):\n",
        "    self.model = model\n",
        "    self.dsg = dataset_generator\n",
        "    self.return_losses = return_losses\n",
        "    self.losses = []\n",
        "    self.verbose = verbose\n",
        "    self.visual_output = visual_output\n",
        "    self.factor = factor\n",
        "    self.b = b\n",
        "\n",
        "  def loss_gradient(self):\n",
        "    loss_fn = Loss(model = self.model, d = d, b=self.b)\n",
        "    with tf.GradientTape(persistent=True) as loss_tape:\n",
        "      loss = loss_fn.total_criterion(self.t, self.x, self.eta, self.t_T, self.x_T, self.eta_T, factor = self.factor)\n",
        "    return loss, loss_tape.gradient(loss, self.model.trainable_variables)\n",
        "\n",
        "  def step(self, optimizer):\n",
        "\n",
        "    '''\n",
        "    A single step in the training regime of the neural network.\n",
        "    '''\n",
        "\n",
        "    loss, loss_grad = self.loss_gradient()\n",
        "\n",
        "    if self.verbose:\n",
        "      self.avg_losses.append(loss.numpy())\n",
        "\n",
        "    if self.return_losses:\n",
        "      self.losses.append(loss)\n",
        "\n",
        "    optimizer.apply_gradients(zip(loss_grad, self.model.trainable_variables))\n",
        "    return self.model\n",
        "\n",
        "  def train_nn(self, epochs, steps_per_epoch, learning_rate = 1e-3, verbose=False):\n",
        "\n",
        "    '''\n",
        "    The main training function to train the neural network.\n",
        "    '''\n",
        "\n",
        "    print('Training the DGM network.')\n",
        "\n",
        "    lr_fn = tf.optimizers.schedules.PolynomialDecay(initial_learning_rate=learning_rate, decay_steps = int(epochs*steps_per_epoch),\n",
        "                                                    end_learning_rate=1e-6, power = 0.9)\n",
        "    opt = tf.keras.optimizers.Adam(lr_fn)\n",
        "\n",
        "    for m in range(epochs):\n",
        "\n",
        "      self.avg_losses = []\n",
        "\n",
        "      self.dsg.generate_dataset()\n",
        "      self.dsg.oversampling()\n",
        "      self.t, self.x, self.eta, self.t_T, self.x_T, self.eta_T, = self.dsg.data_to_tensors()\n",
        "\n",
        "      for step in range(steps_per_epoch):\n",
        "\n",
        "        self.model = self.step(opt)\n",
        "\n",
        "      if self.verbose:\n",
        "        print(f'Avg loss for epoch {m} was: {np.mean(self.avg_losses)}')\n",
        "\n",
        "\n",
        "    self.model.save_weights('dgm_weights')\n",
        "\n",
        "    if self.return_losses:\n",
        "      return self.model, self.losses\n",
        "    return self.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNlCaS-l58sR",
        "outputId": "2af657c9-f41c-470e-921e-2df2e6d6aae3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the DGM network.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7af58eb797e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7af58eb797e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        }
      ],
      "source": [
        "trainer = Train(model = DGMModel([d+2,50,50,50]), dataset_generator = DGMDatasetGenerator4(d=d), factor = 10., verbose=False)\n",
        "trainer.train_nn(epochs = 1, steps_per_epoch=2, learning_rate = 1e-3)\n",
        "# save whole model\n",
        "trainer.model.save(f'model_tf', save_format = 'tf')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below are the plotting functions used for the DGME in the paper."
      ],
      "metadata": {
        "id": "BqX_joNGAKBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import cm\n",
        "import seaborn as sns\n",
        "import imageio\n",
        "import copy\n",
        "import matplotlib as mpl\n",
        "import plotly.graph_objects as go"
      ],
      "metadata": {
        "id": "vi12NW0ZbinO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below there are two visualization classes, Viz2 and VizPropagation."
      ],
      "metadata": {
        "id": "FOhBFZwKAHw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Viz2:\n",
        "  def __init__(self, model, num_measure_points =1_000):\n",
        "    self.model = model\n",
        "    self.num_points = num_measure_points\n",
        "\n",
        "\n",
        "  def single_graph_population_updated(self, t=0., x=0.):\n",
        "    '''\n",
        "    This function gets the values from the neural network, populated on the simplex. Only for d=2.\n",
        "    '''\n",
        "    two_simplex = np.linspace(start = 0, stop = 1, num = self.num_points, endpoint = True)\n",
        "    eta_data = np.zeros((self.num_points, d))\n",
        "    eta_data[:,0] = two_simplex\n",
        "    eta_data[:,1] = 1. - two_simplex\n",
        "    eta_data = tf.convert_to_tensor(eta_data, dtype='float32')\n",
        "    t = tf.fill((self.num_points,), t)\n",
        "    x = tf.fill((self.num_points,), x)\n",
        "    y = self.model(t,x,eta_data)\n",
        "    y_for_graph = y.numpy()[:,0]\n",
        "    return two_simplex, y_for_graph\n",
        "\n",
        "  def display_single_graph_updated(self, t=0., x=0.):\n",
        "    '''\n",
        "    Uses the prior function to graph the DGME neural network's values. Only for d=2.\n",
        "    '''\n",
        "    two_simplex, y_for_graph = self.single_graph_population_updated(t=t, x=x)\n",
        "    fig, ax = plt.subplots(figsize=(6, 4)) #, tight_layout=True)\n",
        "    ax.set_ylim([0,1])\n",
        "    ax.plot(two_simplex, y_for_graph)\n",
        "    ax.set_xlabel(f'$\\mu(x=1)$')\n",
        "    ax.set_ylabel(f'$U(t={round(t,2)},x={int(x)+1},\\eta=\\mu)$')\n",
        "    ax.set_title(r'')\n",
        "    ax.plot(two_simplex, y_for_graph, color = 'black')\n",
        "    return\n",
        "\n",
        "\n",
        "  def display_graph_rainbow_updated(self, x=0., num_times = 15, T = .5):\n",
        "    '''\n",
        "    Displays the DGME result but at multiple times. Only for d=2.\n",
        "    '''\n",
        "    fig, ax = plt.subplots(figsize=(6, 4)) #, tight_layout=True)\n",
        "    ax.set_ylim([-0.01,0.34])\n",
        "    ax.set_xlabel(f'$\\mu(x=1)$')\n",
        "    ax.set_ylabel('')\n",
        "    ax.set_title(f'$U(x={int(x)+1},\\eta=\\mu)$')\n",
        "\n",
        "    time_points = np.linspace(0,T, num = num_times, endpoint = True)\n",
        "\n",
        "    color = cm.rainbow(time_points)\n",
        "\n",
        "    for k, c in enumerate(color):\n",
        "      two_simplex, y_for_graph = self.single_graph_population_updated(t=time_points[k], x=x)\n",
        "      if k == 0:\n",
        "        ax.set_ylim([-0.01,max(y_for_graph)*1.05])\n",
        "      ax.plot(two_simplex, y_for_graph, color = c, label = f't={round(time_points[k],2)}')\n",
        "\n",
        "    ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
        "    return\n",
        "\n",
        "  def display_heatmap(self, num_points = 25, t=0., x=0.):\n",
        "\n",
        "    '''\n",
        "    A specific function to graph the DGME network's values in the case d=3.\n",
        "    '''\n",
        "    # create numpy array of inputs\n",
        "    num_array = int(num_points*(num_points+3)/2.) + 1\n",
        "    eta_data = np.zeros((num_array, 3))\n",
        "    k=0\n",
        "    num_round = 0\n",
        "    for i in range(num_points + 1):\n",
        "      for j in range(num_points - i + 1):\n",
        "        eta1 = float(i)/num_points\n",
        "        eta2 = float(j)/num_points\n",
        "        eta3 = 1. - eta1 - eta2\n",
        "        eta_data[k, 0] = eta1\n",
        "        eta_data[k, 1] = eta2\n",
        "        eta_data[k, 2] = eta3\n",
        "        k+=1\n",
        "    eta_data = tf.convert_to_tensor(eta_data, dtype='float32')\n",
        "    t_data = tf.fill((num_array,), t)\n",
        "    x_data = tf.fill((num_array,), x)\n",
        "    output = self.model(t_data, x_data, eta_data)\n",
        "\n",
        "    grid_data = np.zeros((num_points+1, num_points+1))\n",
        "    np_out = output.numpy()[:,0]\n",
        "    i = num_points\n",
        "    j = 0\n",
        "    for k in range(num_array):\n",
        "      grid_data[i,j] = np_out[k]\n",
        "      if j == i:\n",
        "        i -= 1\n",
        "        j = 0\n",
        "      else:\n",
        "        j += 1\n",
        "\n",
        "    skip = 3\n",
        "    x_labels = []\n",
        "    for i in range(grid_data.shape[0]):\n",
        "      if i % skip == 0:\n",
        "        val = round(i/(grid_data.shape[0]-1),2)\n",
        "      else:\n",
        "        val = ''\n",
        "      x_labels.append(val)\n",
        "    y_labels = []\n",
        "    for i in range(grid_data.shape[1]):\n",
        "      if i % skip == 0:\n",
        "        val = round(1.- i/(grid_data.shape[1]-1),2)\n",
        "      else:\n",
        "        val = ''\n",
        "      y_labels.append(val)\n",
        "\n",
        "    mask = np.triu(np.ones_like(grid_data, dtype=bool))\n",
        "\n",
        "    # x_labels = [round(i/(grid_data.shape[0]-1),2) for i in range(0,grid_data.shape[0],2)]\n",
        "    # y_labels = [round(1.- i/(grid_data.shape[1]-1),2) for i in range(0, grid_data.shape[1],2)]\n",
        "    ax = sns.heatmap(grid_data, mask=mask, xticklabels=x_labels, yticklabels=y_labels, vmin=0, vmax=0.225, cmap=\"PiYG\")\n",
        "\n",
        "    # decrease density of tick labels\n",
        "    for index, label in enumerate(ax.get_xticklabels()):\n",
        "      if index % 2 == 0:\n",
        "          label.set_visible(True)\n",
        "      else:\n",
        "          label.set_visible(False)\n",
        "\n",
        "    for index, label in enumerate(ax.get_yticklabels()):\n",
        "      if index % 2 == 0:\n",
        "          label.set_visible(True)\n",
        "      else:\n",
        "          label.set_visible(False)\n",
        "\n",
        "    ax.set_xlabel(f'$\\mu(x=2)$')\n",
        "    ax.set_ylabel('$\\mu(x=1)$')\n",
        "    ax.set_title(f'$U(t={round(t,2)}, x={int(x)+1},\\eta=\\mu)$')\n",
        "    plt.show()\n",
        "\n",
        "    return ax\n",
        "\n",
        "  def heatmap_timelapse(self, num_points = 25, time_steps = 100, T=0.5, x=0.):\n",
        "    '''\n",
        "    Make a gif by using the d=3 values for multiple times.\n",
        "    '''\n",
        "    images = []\n",
        "    for t_bar in range(time_steps):\n",
        "      t = (t_bar / (time_steps - 1) ) * T\n",
        "      plt.clf()\n",
        "      hm = self.display_heatmap(num_points = 25, t=t, x=0.)\n",
        "\n",
        "      fig = hm.get_figure()\n",
        "      fig.savefig(f\"hm_{t_bar}.png\")\n",
        "      images.append(imageio.imread(f\"hm_{t_bar}.png\"))\n",
        "    imageio.mimsave('hm.gif',images,fps=55)\n",
        "    return"
      ],
      "metadata": {
        "id": "gqj9hhmnAwwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VizPropagation:\n",
        "\n",
        "  def __init__(self, model, d=3):\n",
        "    self.model = model\n",
        "    self.a = 2\n",
        "    self.au = 3\n",
        "    self.al = 1\n",
        "    self.b = 4\n",
        "    self.d = d\n",
        "\n",
        "  def a_star(self, numerator):\n",
        "    return self.a + ( numerator / ((self.au - self.al) * self.b) )\n",
        "\n",
        "  def uniformly_random_measure(self):\n",
        "    pre_normalized = np.random.exponential(1, size = (self.d,))\n",
        "    return pre_normalized / sum(pre_normalized)\n",
        "\n",
        "  def get_measure_points(self, delta_t, T, initial_eta):\n",
        "\n",
        "    '''\n",
        "    This function uses the network and the Fokker--Planck equation to get the corresponding points of the mean field equilibrium.\n",
        "    '''\n",
        "    num_steps = int(T/delta_t)+1\n",
        "    measure_data = np.zeros((num_steps,self.d))\n",
        "    current_time = 0\n",
        "    for step in range(num_steps):\n",
        "      # each step propagate by the Kolmogorov equation\n",
        "      if (step == 0):\n",
        "        past_mu = initial_eta\n",
        "        measure_data[0, :] = past_mu\n",
        "      else:\n",
        "        past_mu = measure_data[step-1,:]\n",
        "\n",
        "        U_vals = []\n",
        "        for x in range(self.d):\n",
        "          t_data = tf.cast(tf.fill((1,), current_time), dtype = 'float32')\n",
        "          x_data = tf.cast(tf.fill((1,), x), dtype = 'float32')\n",
        "          eta_data = tf.expand_dims(input = tf.convert_to_tensor(past_mu, dtype='float32'), axis = 0)\n",
        "          U_x = self.model(t_data, x_data, eta_data).numpy()[:,0]\n",
        "          U_vals.append(U_x)\n",
        "\n",
        "        for x in range(self.d):\n",
        "          temp = 0\n",
        "          for y in range(self.d):\n",
        "            if y != x:\n",
        "              gamma = self.a_star(U_vals[y] - U_vals[x])\n",
        "              temp  += ((past_mu[y]-past_mu[x]) * gamma * delta_t)\n",
        "          measure_data[step, x] = (copy.copy(past_mu[x]) + temp)\n",
        "\n",
        "      current_time += delta_t\n",
        "    return measure_data\n",
        "\n",
        "  def get_value_points(self, delta_t, T, initial_eta):\n",
        "    '''\n",
        "    Along the path from the previous function, this function gives the corresponding value.\n",
        "    '''\n",
        "    num_steps = int(T/delta_t)+1\n",
        "    measure_data = np.zeros((num_steps,self.d))\n",
        "    current_time = 0\n",
        "\n",
        "    measure_data = self.get_measure_points(delta_t=delta_t, T=T, initial_eta=initial_eta)\n",
        "    value_data = np.zeros_like(measure_data)\n",
        "\n",
        "    for step in range(num_steps):\n",
        "      if (step == 0):\n",
        "        past_mu = initial_eta\n",
        "        measure_data[0, :] = past_mu\n",
        "      else:\n",
        "        past_mu = measure_data[step-1,:]\n",
        "      for x in range(self.d):\n",
        "        t_data = tf.cast(tf.fill((1,), current_time), dtype = 'float32')\n",
        "        x_data = tf.cast(tf.fill((1,), x), dtype = 'float32')\n",
        "        eta_data = tf.expand_dims(input = tf.convert_to_tensor(past_mu, dtype='float32'), axis = 0)\n",
        "        U_x = self.model(t_data, x_data, eta_data).numpy()[:,0]\n",
        "        value_data[step,x] = U_x\n",
        "\n",
        "      current_time += delta_t\n",
        "    return measure_data, value_data\n",
        "\n",
        "  def plot_mf_evolution(self, delta_t, T, initial_eta):\n",
        "    '''\n",
        "    A plotting function for the mean field equilibrium.\n",
        "    '''\n",
        "    fig, ax = plt.subplots(figsize=(6, 4)) #, tight_layout=True)\n",
        "    ax.set_ylim([0,1.])\n",
        "    if self.d > 3:\n",
        "      ax.set_ylim([0,max(initial_eta)+0.05])\n",
        "    ax.set_xlabel('$t$')\n",
        "    ax.set_ylabel('$\\mu(t,x)$')\n",
        "\n",
        "    title_string = 'Initial State: ('\n",
        "    for k in range(len(initial_eta)):\n",
        "      title_string += str(round(initial_eta[k],2))\n",
        "      if k < len(initial_eta) - 1:\n",
        "        title_string += ', '\n",
        "      else:\n",
        "        title_string += ')'\n",
        "    ax.set_title(title_string)\n",
        "\n",
        "    num_steps = int(T/delta_t)+1\n",
        "\n",
        "    states = np.linspace(0, self.d, num = self.d, endpoint = False)\n",
        "    time_steps = np.linspace(0, T, num = num_steps, endpoint = True)\n",
        "\n",
        "    color = mpl.colormaps['Paired'](np.arange(0,self.d))\n",
        "\n",
        "    measure_data = self.get_measure_points(delta_t = delta_t, T=T, initial_eta = initial_eta)\n",
        "\n",
        "    for k, c in enumerate(color):\n",
        "      state_k_data = measure_data[:,k]\n",
        "      ax.plot(time_steps, state_k_data, color = c, label = f'$\\mu(t,{int(k)+1})$')\n",
        "    ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
        "    return\n",
        "\n",
        "  def plot_value_propagation(self, delta_t, T, initial_eta):\n",
        "    '''\n",
        "    A plotting function for the cost of the mean field equilibrium.\n",
        "    '''\n",
        "    fig, ax = plt.subplots(figsize=(6, 4)) #, tight_layout=True)\n",
        "    ax.set_xlabel('$t$')\n",
        "    ax.set_ylabel('$\\mu(t,x)$')\n",
        "\n",
        "    title_string = 'Initial State: ('\n",
        "    for k in range(len(initial_eta)):\n",
        "      title_string += str(round(initial_eta[k],2))\n",
        "      if k < len(initial_eta) - 1:\n",
        "        title_string += ', '\n",
        "      else:\n",
        "        title_string += ')'\n",
        "    ax.set_title(title_string)\n",
        "\n",
        "    num_steps = int(T/delta_t)+1\n",
        "\n",
        "    states = np.linspace(0, self.d, num = self.d, endpoint = False)\n",
        "    time_steps = np.linspace(0, T, num = num_steps, endpoint = True)\n",
        "\n",
        "    color = mpl.colormaps['Paired'](np.arange(0,self.d))\n",
        "\n",
        "    measure_data, value_data = self.get_value_points(delta_t = delta_t, T=T, initial_eta = initial_eta)\n",
        "\n",
        "    ax.set_ylim([0,0.35])\n",
        "\n",
        "    for k, c in enumerate(color):\n",
        "      state_k_data = value_data[:,k]\n",
        "      ax.plot(time_steps, state_k_data, color = c, label = f'$U(t,{int(k)+1},\\mu(t))$')\n",
        "    ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
        "    return\n",
        "\n",
        "  def plot_mf_on_simplex(self, delta_t, T, initial_eta):\n",
        "    # ONLY for dimension d=3\n",
        "    '''\n",
        "    A plotting function for the mean field equilibrium, only for d=3.\n",
        "    '''\n",
        "    num_steps = int(T/delta_t)+1\n",
        "    time_steps = np.linspace(0, T, num = num_steps, endpoint = True)\n",
        "    measure_data = self.get_measure_points(delta_t = delta_t, T=T, initial_eta = initial_eta)\n",
        "    x, y, z = measure_data[:,0], measure_data[:,1], measure_data[:,2]\n",
        "\n",
        "    fig = go.Figure(data=[go.Scatter3d(\n",
        "        x=x,\n",
        "        y=y,\n",
        "        z=z,\n",
        "        mode='markers',\n",
        "        marker=dict(\n",
        "            size=3,\n",
        "            color=z,                # set color to an array/list of desired values\n",
        "            colorscale='Viridis',   # choose a colorscale\n",
        "            opacity=0.95\n",
        "        )\n",
        "    )])\n",
        "\n",
        "    # tight layout\n",
        "    fig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\n",
        "    fig.update_layout(\n",
        "    scene = dict(\n",
        "        xaxis = dict(range=[0,1],),\n",
        "                     yaxis = dict(range=[0,1],),\n",
        "                     zaxis = dict(range=[0,1],),),\n",
        "    width=700,\n",
        "    margin=dict(r=20, l=10, b=10, t=10))\n",
        "\n",
        "    x = np.outer(np.linspace(0, 1, 30), np.ones(30))\n",
        "    y = x.copy().T\n",
        "    z = (1-x) - y\n",
        "    fig.add_trace(go.Surface(x=x, y=y, z=z, colorscale = 'gray', opacity = 0.3, showscale = False))\n",
        "    fig.show()\n",
        "    return"
      ],
      "metadata": {
        "id": "NciU3HY0Aytb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following give some examples of how to implement the Viz2 and VizPropagation classes:"
      ],
      "metadata": {
        "id": "EJb0tBxXFGj2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the d=2, rainbow graph:"
      ],
      "metadata": {
        "id": "1fSrvgenFYeb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# d=2\n",
        "# loaded_model = keras.models.load_model('LOCAL PATH/Most_Models/i_dgm_max_2')\n",
        "# loaded_model.compile()\n",
        "# viz = Viz2(model = loaded_model)\n",
        "# viz.display_graph_rainbow_updated(T=0.5)"
      ],
      "metadata": {
        "id": "yOeyiXAjA57-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting the cost along the mean field equilibrium over time:"
      ],
      "metadata": {
        "id": "-uIvxgdqFdw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loaded_model = keras.models.load_model('LOCAL PATH/Most_Models/i_dgm_max_2')\n",
        "# loaded_model.compile()\n",
        "# vizp = VizPropagation(model = loaded_model, d=2)\n",
        "\n",
        "# for eta1 in [0.1 * k for k in range(1,6,1)]:\n",
        "#   vizp.plot_value_propagation(delta_t = 0.005, T=0.5, initial_eta = [eta1,1.-eta1])"
      ],
      "metadata": {
        "id": "tbOFkN08A_DI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The special dimension 3 plot of the mean field equilibrium on the simplex:"
      ],
      "metadata": {
        "id": "CP2K7r1eFjnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loaded_model = keras.models.load_model('LOCAL PATH/Most_Models/i_dgm_max_3')\n",
        "# loaded_model.compile()\n",
        "# vizp = VizPropagation(model = loaded_model)\n",
        "# vizp.plot_mf_on_simplex(delta_t = 0.005, T = 0.5, initial_eta = [0.7, 0.2, 0.1])"
      ],
      "metadata": {
        "id": "VXYym97QBBXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The d=3 heatmaps:"
      ],
      "metadata": {
        "id": "H96o1aBPFn2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# d=3\n",
        "# loaded_model = keras.models.load_model('LOCAL PATH/Most_Models/i_dgm_max_3')\n",
        "# loaded_model.compile()\n",
        "# viz = Viz2(model = loaded_model)\n",
        "# plt.clf()\n",
        "# for t in [0., 0.1, 0.2, 0.3, 0.4, 0.5]:\n",
        "#   viz.display_heatmap(num_points = 60, t=t, x=0.)"
      ],
      "metadata": {
        "id": "ulIhAhfRBRoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rJOohTK5CiSn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
