{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This code implements the DBME method proposed and analyzed in the paper ``DEEP BACKWARD AND GALERKIN METHODS FOR LEARNING FINITE\n",
        "STATE MASTER EQUATIONS'' by Asaf Cohen, Mathieu Laurière and Ethan Zell.\n",
        "\n",
        "The example solved here corresponds to Example 7.1 in the paper."
      ],
      "metadata": {
        "id": "Iu5ID6aTCHOh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42iWzHqiJOsl"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import math, pandas, sklearn, keras, random, copy, time\n",
        "import matplotlib.pyplot as plt\n",
        "tf.random.set_seed(703)\n",
        "np.random.seed(703)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Global, static parameters\n",
        "d=2"
      ],
      "metadata": {
        "id": "WA1EHLiVmBk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DBDatasetGenerator:\n",
        "  '''\n",
        "  This class generates the dataset for the DGM.\n",
        "  '''\n",
        "  def __init__(self, d=2, horizon=.5):\n",
        "    self.d = d\n",
        "    self.horizon = horizon\n",
        "\n",
        "  def uniformly_random_measure(self):\n",
        "    '''\n",
        "    Uses exponential random variables to generate a uniformly random probability vector.\n",
        "    '''\n",
        "    pre_normalized = np.random.exponential(1, size = (self.d,))\n",
        "    return pre_normalized / sum(pre_normalized)\n",
        "\n",
        "  def uniformly_random_measure_vec(self, samples):\n",
        "    '''\n",
        "    Vectorizes the prior function.\n",
        "    '''\n",
        "    data = np.zeros((samples, self.d))\n",
        "    for k in range(samples):\n",
        "      data[k,:] = self.uniformly_random_measure()\n",
        "    return data\n",
        "\n",
        "  def generate_dataset(self, samples=1000):\n",
        "    '''\n",
        "    Creates the dataset which, for the DBME, needs only x and eta data.\n",
        "    '''\n",
        "    x_data = np.random.choice([float(m) for m in range(self.d)],size=samples)\n",
        "    eta_data = self.uniformly_random_measure_vec(samples = samples)\n",
        "    self.x_data = x_data\n",
        "    self.eta_data = eta_data\n",
        "    return x_data, eta_data\n",
        "\n",
        "  def oversample_eta_bijection(self, x):\n",
        "    return (2.* x) - 0.5\n",
        "\n",
        "  def oversampling(self, oversample_eta = True, oversample_T = True):\n",
        "    '''\n",
        "    Apply this function after generate_dataset to modify the domain of the sampled eta and T.\n",
        "    Sampling outside the domain may improve performance along the domain's boundary.\n",
        "    '''\n",
        "    if oversample_eta:\n",
        "      self.eta_data = self.oversample_eta_bijection(self.eta_data)\n",
        "    return self.x_data, self.eta_data\n",
        "\n",
        "  def data_to_tensors(self):\n",
        "    self.x_data = tf.convert_to_tensor(self.x_data, dtype = 'float32')\n",
        "    self.eta_data = tf.convert_to_tensor(self.eta_data, dtype = 'float32')\n",
        "    return self.x_data, self.eta_data"
      ],
      "metadata": {
        "id": "Y8yrKwj3JbwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DBMFGModel(tf.keras.Model):\n",
        "  '''\n",
        "  This class defines the neural network model.\n",
        "  '''\n",
        "  def __init__(self, architecture):\n",
        "    super(DBMFGModel, self).__init__()\n",
        "    self.architecture = architecture # you can give a list specifying the number of nodes in each dense layer\n",
        "    self.layer_list = []\n",
        "\n",
        "    for i,number_of_nodes in enumerate(architecture):\n",
        "      if i == 0:\n",
        "        self.layer_list.append(tf.keras.layers.Dense(units=number_of_nodes, activation='sigmoid',\n",
        "                                                              kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.),\n",
        "                                                              bias_initializer='zeros'))\n",
        "      else:\n",
        "        self.layer_list.append(tf.keras.layers.Dense(units=number_of_nodes, activation='sigmoid',\n",
        "                                                              kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.),\n",
        "                                                              bias_initializer='zeros'))\n",
        "    self.layer_list.append(tf.keras.layers.Dense(units=1, activation = 'elu'))\n",
        "\n",
        "  def call(self, x, eta):\n",
        "    x = tf.expand_dims(x, axis = -1)\n",
        "    input = tf.concat([x, eta], 1)\n",
        "    result = input\n",
        "    for layer in self.layer_list:\n",
        "      result = layer(result)\n",
        "    return result"
      ],
      "metadata": {
        "id": "B1k8FH5lLHE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall from the paper that we are interested in approximately solving the master equation:\n",
        "\n",
        "$$\n",
        "\\partial_t U(t,x,\\eta) = H(x,\\Delta_x U(t,\\cdot,\\eta))+ F(x,\\eta) + \\sum_{y,z\\in [d]} D^\\eta_{yz} U(t,x,\\eta) \\gamma^*_z(y,\\Delta_y U(t,\\cdot,\\eta)) \\eta_y,\n",
        "$$\n",
        "\n",
        "where in this example:\n",
        "\n",
        "$$F(x,\\eta) = \\eta_x,$$\n",
        "\n",
        "$$H(x,p) := \\min_{a} \\Big\\{\\frac{1}{2}|a|^2 + a\\cdot p\\Big\\},$$\n",
        "\n",
        "and where $\\gamma^*$ is the associated minimal argument that minimizes the Hamiltonian $H$. Recall that $\\Delta_x b:= (b_y - b_x)_{y\\in [d]}$ is a finite difference vector and $D^\\eta_{yz}$ denotes the directional derivative in the $z$ minus $y$ direction (in terms of the standard basis).\n",
        "\n",
        "In the Loss class below, $F$ is referred to as the mean_field_cost and $H$ is the Hamiltonian."
      ],
      "metadata": {
        "id": "X09gbaQ9DUkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Loss():\n",
        "  '''\n",
        "  This class defines the loss, which involves the MFG model.\n",
        "  '''\n",
        "  def __init__(self, model_mesh, step, partition_step, future_is_terminal = False, a = 2., a_l = 1., a_u = 3., b = 4.):\n",
        "    self.model = model_mesh[step]\n",
        "\n",
        "    if future_is_terminal:\n",
        "      self.model_at_future_step = None\n",
        "    else:\n",
        "      self.model_at_future_step = model_mesh[step + 1]\n",
        "\n",
        "    self.a = a\n",
        "    self.a_l = a_l\n",
        "    self.a_u = a_u\n",
        "    self.b = b\n",
        "    self.partition_step = partition_step\n",
        "    self.future_is_terminal = future_is_terminal\n",
        "    return\n",
        "\n",
        "  def a_star(self, numerator):\n",
        "    '''\n",
        "    The computed value of $\\gamma^*$, the minimal argument of the Hamiltonian. The paper derives this formula explicitly.\n",
        "    '''\n",
        "    return (numerator / ((self.a_u - self.a_l) * self.b) ) + self.a\n",
        "\n",
        "  def Hamiltonian(self, output, complement_output):\n",
        "    '''\n",
        "    The Hamiltonian, denoted H in the paper.\n",
        "    '''\n",
        "    a_star_x = self.a_star(output - complement_output)\n",
        "    pre_running = a_star_x - (self.a * np.ones(a_star_x.shape))\n",
        "    running_cost = self.b * tf.math.square(pre_running)\n",
        "    change_of_state = tf.multiply(a_star_x, (complement_output - output))\n",
        "    return running_cost + change_of_state\n",
        "\n",
        "  def mean_field_cost(self, x, eta):\n",
        "    '''\n",
        "    The common cost, denoted F in the paper.\n",
        "    '''\n",
        "    mf = np.zeros(x.shape)\n",
        "    for i,entry in enumerate(x):\n",
        "      mf[i] = eta[i, int(entry)]\n",
        "    mfc = tf.convert_to_tensor(mf, dtype='float32')\n",
        "    return mfc\n",
        "\n",
        "  def criterion(self, x, eta):\n",
        "    '''\n",
        "    The loss function for the DBME, which considers the partition step and the neural network at the immediately future time step.\n",
        "    '''\n",
        "    output = tf.squeeze(self.model(x, eta))\n",
        "    if self.future_is_terminal:\n",
        "      future_output = tf.zeros(output.shape)\n",
        "    else:\n",
        "      future_output = tf.squeeze(self.model_at_future_step(x, eta))\n",
        "    complement_output = tf.squeeze(self.model(1.-x, eta))\n",
        "    hamiltonian = self.Hamiltonian(output, complement_output)\n",
        "    mean_field_cost = self.mean_field_cost(x, eta)\n",
        "    loss_sum = future_output - output + (self.partition_step * (hamiltonian + mean_field_cost))\n",
        "    squared_loss = tf.math.square(loss_sum)\n",
        "\n",
        "    return squared_loss\n",
        "\n",
        "  def total_criterion(self, x, eta):\n",
        "    '''\n",
        "    The DBME uses a max norm in its loss.\n",
        "    '''\n",
        "    unreduced_loss = self.criterion(x, eta)\n",
        "    loss = tf.math.reduce_max(unreduced_loss)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "Xx_NFKwJL_nL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Train():\n",
        "  def __init__(self, model_architecture, dataset_generator, partition_step,\n",
        "               oversampling = True, return_losses = False, verbose = False, visual_output = False):\n",
        "    self.model_arch = model_architecture\n",
        "    self.dsg = dataset_generator\n",
        "    self.return_losses = return_losses\n",
        "    self.losses = []\n",
        "    self.verbose = verbose\n",
        "    self.visual_output = visual_output\n",
        "    self.partition_step = partition_step\n",
        "    self.total_steps = int(self.dsg.horizon / self.partition_step) + 1\n",
        "    print(f'Total number of steps will be: {self.total_steps}')\n",
        "    self.nn_mesh = np.zeros(self.total_steps, dtype=object)\n",
        "\n",
        "  def initialize_nn_mesh(self):\n",
        "    # input_shape = (d+1,)\n",
        "    # d dimensions for the measure and the last dimension is for the jump process\n",
        "    for i in range(self.nn_mesh.shape[0]):\n",
        "      self.nn_mesh[i] = DBMFGModel(architecture = self.model_arch)\n",
        "    return self.nn_mesh\n",
        "\n",
        "  def loss_gradient(self, step):\n",
        "    if step >= self.total_steps - 1:\n",
        "      future = True\n",
        "    else:\n",
        "      future = False\n",
        "    loss_fn = Loss(model_mesh = self.nn_mesh, step = step, partition_step = self.partition_step,\n",
        "                   future_is_terminal = future)\n",
        "    with tf.GradientTape(persistent=True) as loss_tape:\n",
        "      loss = loss_fn.total_criterion(self.x, self.eta)\n",
        "    return loss, loss_tape.gradient(loss, self.nn_mesh[step].trainable_variables)\n",
        "\n",
        "  def step(self, step, optimizer):\n",
        "\n",
        "    '''\n",
        "    A single step in the training regime of a particular neural network in the mesh.\n",
        "    '''\n",
        "\n",
        "    loss, loss_grad = self.loss_gradient(step = step)\n",
        "\n",
        "    if self.verbose:\n",
        "      self.avg_losses.append(loss.numpy())\n",
        "    if self.return_losses:\n",
        "      self.losses.append(loss)\n",
        "\n",
        "    optimizer.apply_gradients(zip(loss_grad, self.nn_mesh[step].trainable_variables))\n",
        "    return self.nn_mesh[step]\n",
        "\n",
        "  def train(self, epochs, steps_per_epoch, learning_rate = 1e-4):\n",
        "\n",
        "    '''\n",
        "    The main training function to train each neural network in the mesh, one by one, from the terminal time until time zero.\n",
        "    '''\n",
        "\n",
        "    self.initialize_nn_mesh()\n",
        "    print('Training network mesh.')\n",
        "\n",
        "    for n in tqdm(range(self.total_steps-1, -1, -1)): # start from the terminal time and work backward\n",
        "\n",
        "      if self.verbose:\n",
        "        print('')\n",
        "        print(f'----------training network {n}----------')\n",
        "      if n < self.total_steps - 1:\n",
        "        e = epochs\n",
        "        try:\n",
        "          self.nn_mesh[n].load_weights(f'weights_{n+1}') # we expect that the previous learned weights would be close to the weights in the next step, by continuity of the master equation solution\n",
        "        except:\n",
        "          pass\n",
        "      else:\n",
        "        e = epochs*3\n",
        "\n",
        "      opt = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
        "\n",
        "      for m in tqdm(range(e)):\n",
        "\n",
        "        self.avg_losses = []\n",
        "        self.dsg.generate_dataset()\n",
        "        self.dsg.oversampling()\n",
        "        self.x, self.eta = self.dsg.data_to_tensors()\n",
        "\n",
        "        for s in range(steps_per_epoch):\n",
        "          self.nn_mesh[n] = self.step(step = n, optimizer = opt)\n",
        "\n",
        "        if self.verbose:\n",
        "          print(f'Avg loss for epoch {m} was: {np.mean(self.avg_losses)}')\n",
        "\n",
        "      self.nn_mesh[n].save_weights(f'weights_{n}')\n",
        "\n",
        "    if self.return_losses:\n",
        "      return self.nn_mesh, self.losses\n",
        "    return self.nn_mesh"
      ],
      "metadata": {
        "id": "mKkE1FugP93t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Train(model_architecture = [d+1,20,20], dataset_generator = DBDatasetGenerator(), partition_step = 0.1, oversampling = True, verbose = True)\n",
        "mesh = trainer.train(epochs = 5, steps_per_epoch = 2, learning_rate = 1e-4)\n",
        "# For best results, increase epochs to > 150 and steps_per_epoch > 20."
      ],
      "metadata": {
        "id": "d-CbhDKyVPr1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7da1bd5-fdf4-462a-94d4-b427335a14e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of steps will be: 6\n",
            "Training network mesh.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/6 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----------training network 5----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/15 [00:00<?, ?it/s]\u001b[AWARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7c1a3a6724d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7c1a3a6724d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "\n",
            "  7%|▋         | 1/15 [00:13<03:03, 13.13s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss for epoch 0 was: 0.4934001863002777\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 13%|█▎        | 2/15 [00:16<01:37,  7.53s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss for epoch 1 was: 0.48829707503318787\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 20%|██        | 3/15 [00:19<01:04,  5.38s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss for epoch 2 was: 0.48230621218681335\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 27%|██▋       | 4/15 [00:21<00:45,  4.14s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss for epoch 3 was: 0.47778576612472534\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 33%|███▎      | 5/15 [00:23<00:33,  3.36s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss for epoch 4 was: 0.472836971282959\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 40%|████      | 6/15 [00:25<00:25,  2.87s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss for epoch 5 was: 0.4679490923881531\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 47%|████▋     | 7/15 [00:27<00:20,  2.56s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss for epoch 6 was: 0.4624747633934021\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 53%|█████▎    | 8/15 [00:29<00:16,  2.37s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss for epoch 7 was: 0.45812907814979553\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 60%|██████    | 9/15 [00:31<00:13,  2.29s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss for epoch 8 was: 0.45323896408081055\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 67%|██████▋   | 10/15 [00:34<00:11,  2.31s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss for epoch 9 was: 0.44824522733688354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 73%|███████▎  | 11/15 [00:35<00:08,  2.18s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss for epoch 10 was: 0.4434051513671875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 80%|████████  | 12/15 [00:37<00:06,  2.10s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss for epoch 11 was: 0.43834227323532104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 87%|████████▋ | 13/15 [00:39<00:04,  2.05s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss for epoch 12 was: 0.43380287289619446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 93%|█████████▎| 14/15 [00:41<00:02,  2.00s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss for epoch 13 was: 0.429104745388031\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 15/15 [00:43<00:00,  2.91s/it]\n",
            " 17%|█▋        | 1/6 [00:43<03:38, 43.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss for epoch 14 was: 0.4244789481163025\n",
            "\n",
            "----------training network 4----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:03<00:13,  3.34s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss for epoch 0 was: 0.02587473765015602\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 40%|████      | 2/5 [00:05<00:07,  2.50s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss for epoch 1 was: 0.024771012365818024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 60%|██████    | 3/5 [00:07<00:04,  2.23s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss for epoch 2 was: 0.023161455988883972\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 80%|████████  | 4/5 [00:09<00:02,  2.12s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss for epoch 3 was: 0.022568881511688232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 5/5 [00:11<00:00,  2.22s/it]\n",
            " 33%|███▎      | 2/6 [00:54<01:38, 24.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss for epoch 4 was: 0.021516207605600357\n",
            "\n",
            "----------training network 3----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:02<00:11,  2.79s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss for epoch 0 was: 0.02593083307147026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 40%|████      | 2/5 [00:05<00:07,  2.49s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss for epoch 1 was: 0.02453194372355938\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 60%|██████    | 3/5 [00:07<00:04,  2.24s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss for epoch 2 was: 0.023672379553318024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 80%|████████  | 4/5 [00:08<00:02,  2.13s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss for epoch 3 was: 0.022543584927916527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 5/5 [00:10<00:00,  2.19s/it]\n",
            " 50%|█████     | 3/6 [01:05<00:55, 18.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss for epoch 4 was: 0.021487537771463394\n",
            "\n",
            "----------training network 2----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:02<00:10,  2.65s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss for epoch 0 was: 0.02591218613088131\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 40%|████      | 2/5 [00:04<00:07,  2.41s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss for epoch 1 was: 0.024855801835656166\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 60%|██████    | 3/5 [00:07<00:04,  2.32s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss for epoch 2 was: 0.023754045367240906\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 80%|████████  | 4/5 [00:09<00:02,  2.17s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss for epoch 3 was: 0.022692430764436722\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 5/5 [00:10<00:00,  2.20s/it]\n",
            " 67%|██████▋   | 4/6 [01:16<00:30, 15.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss for epoch 4 was: 0.02161579579114914\n",
            "\n",
            "----------training network 1----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:02<00:10,  2.68s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss for epoch 0 was: 0.02596568875014782\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 40%|████      | 2/5 [00:04<00:06,  2.25s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss for epoch 1 was: 0.024588678032159805\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 60%|██████    | 3/5 [00:06<00:04,  2.24s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss for epoch 2 was: 0.023714661598205566\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 80%|████████  | 4/5 [00:09<00:02,  2.23s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss for epoch 3 was: 0.022486906498670578\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 5/5 [00:11<00:00,  2.21s/it]\n",
            " 83%|████████▎ | 5/6 [01:27<00:13, 13.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss for epoch 4 was: 0.02165970951318741\n",
            "\n",
            "----------training network 0----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:02<00:10,  2.72s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss for epoch 0 was: 0.02591516822576523\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 40%|████      | 2/5 [00:05<00:08,  2.76s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss for epoch 1 was: 0.024617256596684456\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 60%|██████    | 3/5 [00:07<00:04,  2.43s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss for epoch 2 was: 0.023756545037031174\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 80%|████████  | 4/5 [00:09<00:02,  2.41s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss for epoch 3 was: 0.022652970626950264\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 5/5 [00:11<00:00,  2.38s/it]\n",
            "100%|██████████| 6/6 [01:39<00:00, 16.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss for epoch 4 was: 0.021581776440143585\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below are the plotting functions used for the DBME in the paper."
      ],
      "metadata": {
        "id": "q4vxWWQQbhVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.pyplot import cm\n",
        "import seaborn as sns\n",
        "import imageio\n",
        "import copy\n",
        "import matplotlib as mpl\n",
        "import plotly.graph_objects as go"
      ],
      "metadata": {
        "id": "IppI2XqFbfcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two visualization classes. The first allows comparison of one output of the DGME method with one output of the DBME method. To get the plot comparing multiple partition sizes for the DBME output to the DGME output, we use the class VizSeveral in the latter cell. As noted in the paper, in practice, the models were computed on Michigan's Great Lakes advanced computing cluster, downloaded, and then loaded into these visualization classes.\n",
        "\n",
        "In the first class, VizBoth, the second input is a list of neural networks corresponding to the mesh output of a single DBME run. However, in VizSeveral, the input is an array of neural networks, corresponding to several meshes, and several DBME runs with different partition steps."
      ],
      "metadata": {
        "id": "kG3Sft6IcgkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VizBoth:\n",
        "  def __init__(self, dgm_model, dbmfg_model_list, num_measure_points =1_000):\n",
        "    self.dgm_model = dgm_model\n",
        "    self.dbmfg_model_list = dbmfg_model_list\n",
        "    self.num_points = num_measure_points\n",
        "\n",
        "  def single_dgm_graph_population(self, t=0., x=0.):\n",
        "    two_simplex = np.linspace(start = 0, stop = 1, num = self.num_points, endpoint = True)\n",
        "    data_for_graph = np.zeros((self.num_points, d+2))\n",
        "    eta1 = tf.convert_to_tensor(two_simplex, dtype='float32')\n",
        "    eta2 = 1.- eta1\n",
        "    t = tf.fill((self.num_points,), t)\n",
        "    x = tf.fill((self.num_points,), x)\n",
        "    y = self.dgm_model(t,x,eta1,eta2)\n",
        "    y_for_graph = y.numpy()[:,0]\n",
        "    return two_simplex, y_for_graph\n",
        "\n",
        "  def single_dgm_graph_population_updated(self, t=0., x=0.):\n",
        "    two_simplex = np.linspace(start = 0, stop = 1, num = self.num_points, endpoint = True)\n",
        "    eta_data = np.zeros((self.num_points, d))\n",
        "    eta_data[:,0] = two_simplex\n",
        "    eta_data[:,1] = 1. - two_simplex\n",
        "    eta_data = tf.convert_to_tensor(eta_data, dtype='float32')\n",
        "    t = tf.fill((self.num_points,), t)\n",
        "    x = tf.fill((self.num_points,), x)\n",
        "    y = self.dgm_model(t,x,eta_data)\n",
        "    y_for_graph = y.numpy()[:,0]\n",
        "    return two_simplex, y_for_graph\n",
        "\n",
        "  def single_dbmfg_graph_population(self, model_number=0, x=0.):\n",
        "    two_simplex = np.linspace(start = 0, stop = 1, num = self.num_points, endpoint = True)\n",
        "    data_for_graph = np.zeros((self.num_points, d+2))\n",
        "    eta_data = np.zeros((len(two_simplex), d))\n",
        "    eta_data[:,0] = two_simplex\n",
        "    eta_data[:,1] = 1.-two_simplex\n",
        "    eta_data = tf.convert_to_tensor(eta_data, dtype='float32')\n",
        "    x = tf.fill((self.num_points,), x)\n",
        "    y = self.dbmfg_model_list[model_number](x,eta_data)\n",
        "    y_for_graph = y.numpy()[:,0]\n",
        "    return two_simplex, y_for_graph\n",
        "\n",
        "  def display_single_dgm_graph(self, t=0., x=0.):\n",
        "    two_simplex, y_for_graph = self.single_dgm_graph_population(t=t, x=x)\n",
        "    fig, ax = plt.subplots(figsize=(6, 4)) #, tight_layout=True)\n",
        "    ax.set_ylim([0,1])\n",
        "    ax.plot(two_simplex, y_for_graph)\n",
        "    ax.set_xlabel(f'$\\mu(x=0)$')\n",
        "    ax.set_ylabel(f'$U(t={round(t,2)},x={int(x)},\\eta=\\mu)$')\n",
        "    ax.set_title(r'')\n",
        "    ax.plot(two_simplex, y_for_graph, color = 'black')\n",
        "    return\n",
        "\n",
        "  def display_single_dbmfg_graph(self, model_number=0, x=0.):\n",
        "    two_simplex, y_for_graph = self.single_dbmfg_graph_population(model_number = model_number, x=x)\n",
        "    fig, ax = plt.subplots(figsize=(6, 4)) #, tight_layout=True)\n",
        "    ax.set_ylim([0,1])\n",
        "    ax.plot(two_simplex, y_for_graph)\n",
        "    ax.set_xlabel(f'$\\mu(x=0)$')\n",
        "    ax.set_ylabel(f'$U(t=,x={int(x)},\\eta=\\mu)$')\n",
        "    ax.set_title(r'')\n",
        "    ax.plot(two_simplex, y_for_graph, color = 'black')\n",
        "    return\n",
        "\n",
        "  def graph_errors_over_time(self, x=0., T=0.5):\n",
        "    # graphing average model difference over time\n",
        "    time_points = np.linspace(0, T, num = len(self.dbmfg_model_list), endpoint = True)\n",
        "    average_differences = np.zeros(len(time_points))\n",
        "    for k,t in enumerate(time_points):\n",
        "      two_simplex, y_dgm = self.single_dgm_graph_population(t=time_points[k], x=x)\n",
        "      two_simplex, y_dbmfg = self.single_dbmfg_graph_population(model_number=k, x=x)\n",
        "      average_differences[k] = np.mean(y_dgm-y_dbmfg)\n",
        "    fig, ax = plt.subplots(figsize=(6, 4)) #, tight_layout=True)\n",
        "    ax.set_ylim([-1,1])\n",
        "    ax.plot(time_points, average_differences)\n",
        "    ax.set_xlabel(f'$t$')\n",
        "    ax.set_ylabel(f'Avg Difference: DGME - DBME')\n",
        "    ax.set_title(r'')\n",
        "    return"
      ],
      "metadata": {
        "id": "8nWZmhDkcWEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VizSeveral:\n",
        "  def __init__(self, dgm_model, dbmfg_model_array = None, num_measure_points =1_000):\n",
        "    self.dgm_model = dgm_model\n",
        "    self.num_points = num_measure_points\n",
        "    self.dbmfg_model_list_of_lists = dbmfg_model_array\n",
        "    self.viz_both_list = []\n",
        "    for dbmfg_model in dbmfg_model_array:\n",
        "      vb = VizBoth(dgm_model=dgm_model, dbmfg_model_list=dbmfg_model)\n",
        "      self.viz_both_list.append(vb)\n",
        "\n",
        "  def display_dgm_graph_rainbow_updated(self, x=0., T = .5):\n",
        "    num_meshes = len(self.dbmfg_model_list_of_lists)\n",
        "    fig, ax = plt.subplots(figsize=(6, 4)) #, tight_layout=True)\n",
        "    ax.set_ylim([-0.3,0.3])\n",
        "    ax.set_xlabel(f'$t$')\n",
        "    ax.set_title(f'Avg Difference: DGME - DBME')\n",
        "\n",
        "    color = cm.rainbow(np.linspace(0,1,len(self.dbmfg_model_list_of_lists)))\n",
        "\n",
        "    avg_diff_list = []\n",
        "    time_pts_list = []\n",
        "    for i, dbmfg_model_list in enumerate(self.dbmfg_model_list_of_lists):\n",
        "      time_points = np.linspace(0, T, num = len(dbmfg_model_list), endpoint = True)\n",
        "      time_pts_list.append(time_points)\n",
        "      average_differences = np.zeros(len(time_points))\n",
        "      for k,t in enumerate(time_points):\n",
        "        two_simplex, y_dgm = self.viz_both_list[i].single_dgm_graph_population_updated(t=time_points[k], x=x)\n",
        "        two_simplex, y_dbmfg = self.viz_both_list[i].single_dbmfg_graph_population(model_number=k, x=x)\n",
        "        average_differences[k] = np.mean(y_dgm-y_dbmfg)\n",
        "      avg_diff_list.append(average_differences)\n",
        "    partition_size = [0.1, 0.05, 0.01, 0.005, 0.001]\n",
        "    for i, avg_diff in enumerate(avg_diff_list):\n",
        "      ax.plot(time_pts_list[i], avg_diff, color = color[i], label = f'Partition step: {partition_size[i]}')\n",
        "    ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
        "    return"
      ],
      "metadata": {
        "id": "6OUC9q8-bl4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is some sample code for how to implement these classes."
      ],
      "metadata": {
        "id": "jC_o7YlqeKWj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading a saved DGME model:"
      ],
      "metadata": {
        "id": "nwLlbk56eeq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loaded_dgm_model = keras.models.load_model('LOCAL PATH/Most_Models/i_dgm_max_2')\n",
        "# loaded_dgm_model.compile()"
      ],
      "metadata": {
        "id": "Jwf9YkqSeIpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading in DBME meshes corresponding to different partition sizes (can take time when loading many)."
      ],
      "metadata": {
        "id": "GQ0_4-58eh4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# num_models = [6, 11, 21]\n",
        "# loaded_dbmfg_model_list_of_lists = []\n",
        "# for i in range(3):\n",
        "#   loaded_dbmfg_model_list = []\n",
        "#   filepath = f'LOCAL PATH/Most_Models/mesh_models{i}'\n",
        "#   for k in range(num_models[i]):\n",
        "#     loaded_dbmfg_model = keras.models.load_model(filepath+'/model_tf_'+str(k))\n",
        "#     loaded_dbmfg_model.compile()\n",
        "#     loaded_dbmfg_model_list.append(loaded_dbmfg_model)\n",
        "#   loaded_dbmfg_model_list_of_lists.append(loaded_dbmfg_model_list)"
      ],
      "metadata": {
        "id": "B6vPSgQ6eIsB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making a VizBoth object and displaying the corresponding graphs:"
      ],
      "metadata": {
        "id": "ti4OfZGKesgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# viz = VizBoth(dgm_model = loaded_dgm_model, dbmfg_model_list = loaded_dbmfg_model_list)\n",
        "# plt.clf()\n",
        "# # viz.display_single_dbmfg_graph(model_number=11)\n",
        "# # viz.display_single_dgm_graph(t=0.25)\n",
        "# viz.graph_errors_over_time()"
      ],
      "metadata": {
        "id": "s4o6cAuBeIuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making a VizSeveral object and displaying its comparative graph:"
      ],
      "metadata": {
        "id": "m-ZYcAxSex51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vizall = VizSeveral(dgm_model = loaded_dgm_model, dbmfg_model_array = loaded_dbmfg_model_list_of_lists)\n",
        "# vizall.display_dgm_graph_rainbow_updated()"
      ],
      "metadata": {
        "id": "ixZL7dFdeXVk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
